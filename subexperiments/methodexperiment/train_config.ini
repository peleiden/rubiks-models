[DEFAULT]
location = data/methods
rollouts = 1500
rollout_depth = 30
rollout_games = 7500
batch_size = 1000
lr = 2e-4
gamma = 0.9
tau = 0.3
update_interval = 100
evaluation_interval = 20
is2024 = True
arch = fc_small
analysis = True

[PAPER]
alpha_update = 0
reward_method = paper

[WEIGHTED]
alpha_update = 0
reward_method = lapanfix

[ADAPTIVE]
alpha_update = 0.025
reward_method = lapanfix 

[UNWEIGHTED]
alpha_update = 1
reward_method = lapanfix

[NOTAU]
alpha_update = 0
reward_method = lapanfix
tau = 1


# Run command
# runtrain.py --config configs/method_train.ini --location data/methodexperiment

# Default configuration values at run
# {'alpha_update': 0,
#  'analysis': 'True',
#  'arch': 'fc_small',
#  'batch_size': '1000',
#  'evaluation_interval': '20',
#  'gamma': '0.9',
#  'is2024': 'True',
#  'location': 'data/methods',
#  'lr': '2e-4',
#  'nn_init': 'glorot',
#  'optim_fn': 'Adam',
#  'reward_method': 'lapanfix',
#  'rollout_depth': '30',
#  'rollout_games': '7500',
#  'rollouts': '1500',
#  'tau': '0.3',
#  'update_interval': '100'}
